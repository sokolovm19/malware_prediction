# C: LGBM(41) - used RF feature extraction.

In this experiment we used RF feature extraction to remove the features that have less than 3% of importance on the dataset. We kept 41 features.

## Steps

1. Preprocess data, using `Random Forest Features.ipynb`.

2. Split the data into **training** and **testing** sets, using `Split train test (41).ipynb`.

   Use 80% of the data for training and 20% for testing.

3. Split the **training** data into five folds, using `Data to 5 files(41).ipynb`.

4. Train LGBM on each of the five folds (`LGBM1(41).ipynb`, ..., `LGBM5(41).ipynb`), evaluate it on **testing** data, and output the predictions to `Prediction1.csv`, ..., `Prediction5.csv`.

5. Combine the predictions into one file, `majority.csv`, using the `majority.sh` script.

6. Generate the evaluation metrics, using `Voting result(41).ipynb`.